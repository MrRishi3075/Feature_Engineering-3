{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826f76e3-9a4e-4137-b8c1-ba38d863f00f",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf0b4b-bf44-4ba8-b78f-a0a64b1606fb",
   "metadata": {},
   "source": [
    "* * Min-Max scaling is a data preprocessing technique used to scale and normalize the features of a dataset within a specific range, typically between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "Xscaled= max(X)竏知in(X)/X竏知in(X)\n",
    "#### where :- \n",
    "* X is the original value, \n",
    "* min(X) is the minimum value in the feature, and \n",
    "* max(X) is the maximum value in the feature.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7e115d-171e-45fa-acd5-df4a080d4969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1.0, 2.0], [5.0, 8.0], [10.0, 7.0]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.         0.        ]\n",
      " [0.44444444 1.        ]\n",
      " [1.         0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "# Example in Python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[1.0, 2.0], [5.0, 8.0], [10.0, 7.0]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9b5b5-a97f-4384-a6b9-5e678d8a9f45",
   "metadata": {},
   "source": [
    "\n",
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efcd659-022b-4b94-adc0-fbe6f0009aa4",
   "metadata": {},
   "source": [
    "* * The Unit Vector technique, also known as Unit Vector normalization or L2 normalization, scales the values of a feature vector to have a magnitude of 1. It is different from Min-Max scaling as it doesn't scale the values to a specific range but focuses on maintaining the direction of the vector while making its magnitude 1.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521f8b5c-a470-4ea3-a90b-580be1a6e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1.0, 2.0], [5.0, 8.0], [10.0, 7.0]]\n",
      "\n",
      "Normalized Data:\n",
      "[[0.4472136  0.89442719]\n",
      " [0.52999894 0.8479983 ]\n",
      " [0.81923192 0.57346234]]\n"
     ]
    }
   ],
   "source": [
    "# Example in Python\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = [[1.0, 2.0], [5.0, 8.0], [10.0, 7.0]]\n",
    "normalized_data = normalize(data, norm='l2')\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712611b0-a626-47a4-b081-d96e2be2dd0c",
   "metadata": {},
   "source": [
    "\n",
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67b442-a351-48d2-b2b5-8400473792fa",
   "metadata": {},
   "source": [
    "* * PCA is a dimensionality reduction technique that transforms the original features of a dataset into a new set of uncorrelated features, called principal components. These components capture the maximum variance in the data. PCA is used to reduce the number of features, making the data more manageable and efficient while retaining as much information as possible.\n",
    "\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719efa96-f9f7-4cee-8769-472c0ae6c87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]\n",
      "\n",
      "Reduced Data:\n",
      "[[-5.19615242e+00  2.56395025e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Example in Python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nReduced Data:\")\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3d121-6571-4c1a-9e3c-248320f894ae",
   "metadata": {},
   "source": [
    "\n",
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248afeb-f4b3-4979-9039-5b7d9de99dde",
   "metadata": {},
   "source": [
    "* * Principal Component Analysis (PCA) is a dimensionality reduction technique that is commonly used for feature extraction. Feature extraction involves transforming the original features of a dataset into a new set of features, which are usually fewer in number and capture the most important information from the original features. PCA achieves this by identifying and emphasizing the directions (principal components) in which the data varies the most.\n",
    "\n",
    "\n",
    "* The relationship between PCA and feature extraction lies in the fact that PCA essentially extracts a new set of features (principal components) that are linear combinations of the original features. These principal components are ordered by the amount of variance they capture, with the first principal component capturing the most variance, the second capturing the second most, and so on.\n",
    "\n",
    "##### Here's an example in Python to illustrate how PCA can be used for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482c84e4-2d81-43f8-b09d-a5f973bac13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "\n",
      "Principal Components:\n",
      "[[-5.19615242e+00  2.56395025e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with three features\n",
    "data = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Initialize PCA with the number of components to retain\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data to obtain the principal components\n",
    "principal_components = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nPrincipal Components:\")\n",
    "print(principal_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067be75-1134-47dd-96b7-d8ad5319db82",
   "metadata": {},
   "source": [
    "\n",
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef05f0-bc47-42ef-90e5-dc7440b4b47b",
   "metadata": {},
   "source": [
    "* * Min-Max scaling is a data preprocessing technique that transforms the features of a dataset to a specific range, typically between 0 and 1. This is done to ensure that all features contribute equally to the model training process, especially when the features have different scales. In the context of your food delivery service recommendation system project, where you have features like price, rating, and delivery time, Min-Max scaling can be applied as follows:\n",
    "\n",
    "#### 1. Understand the Range of Each Feature:\n",
    "\n",
    "* Examine the range of values for each feature in your dataset. For example, price might range from, say, $5 to $50, rating might range from 1 to 5, and delivery time might range from 10 minutes to 60 minutes.\n",
    "\n",
    "#### 2. Define the Scaling Formula:\n",
    "\n",
    "* The Min-Max scaling formula for a feature X is given by:\n",
    "Xscaled= max(X)竏知in(X)/X竏知in(X)\n",
    "\n",
    "##### where :-\n",
    "* X is the original value,\n",
    "* min(X) is the minimum value in the feature, and\n",
    "* max(X) is the maximum value in the feature.\n",
    "\n",
    "#### 3. Apply Min-Max Scaling:\n",
    "\n",
    "* For each feature (price, rating, delivery time), apply the Min-Max scaling formula. This will transform the values of each feature to a range between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32eb1e-cb43-4a2e-a98b-fe009b17ab08",
   "metadata": {},
   "source": [
    "#### 4. Update the Dataset:\n",
    "\n",
    "* Replace the original values of the features with their scaled counterparts in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7c1d1-87b1-4973-a4f4-0a258b375c91",
   "metadata": {},
   "source": [
    "#### 5. Normalization Complete:\n",
    "\n",
    "* * After this process, the features will be normalized and scaled between 0 and 1. This ensures that no single feature dominates the learning process due to having a larger scale than others.\n",
    "\n",
    "* Min-Max scaling helps in achieving uniformity in the scales of different features, making it easier for machine learning models to learn patterns effectively without being biased towards features with larger scales.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e99742-192a-409b-8ef7-817faa68743d",
   "metadata": {},
   "source": [
    "\n",
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b3ae2-fa13-4f66-8fdf-cb047dbdc0e7",
   "metadata": {},
   "source": [
    "* * Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in machine learning and data analysis. In the context of your project to predict stock prices with a dataset containing many features, including company financial data and market trends, here's how you could use PCA to reduce the dimensionality:\n",
    "\n",
    "#### 1. Understand the Dataset:\n",
    "\n",
    "* Start by understanding the structure of your dataset, including the various features and their relevance to predicting stock prices.\n",
    "\n",
    "#### 2. Data Preprocessing:\n",
    "\n",
    "* Standardize or normalize the features to ensure that they are on similar scales. This step is crucial for PCA because it is sensitive to the scale of the features.\n",
    "\n",
    "#### 3. Apply PCA:\n",
    "\n",
    "* Use the PCA algorithm to identify the principal components of the dataset. Principal components are linear combinations of the original features that capture the maximum variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7052025f-5d27-4672-b53a-2ebf3a18c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example feature matrix with numerical features\n",
    "X = np.array([\n",
    "    [1.2, 3.4, 5.6],\n",
    "    [7.8, 9.0, 2.3],\n",
    "    [4.5, 6.7, 8.9]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "511fcb6d-a11d-448c-8cc4-22931dae6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'X' is your feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=k)  # Choose the number of components 'k'\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ba92f-47e6-46a3-b9ec-3eb803b1a80a",
   "metadata": {},
   "source": [
    "* Here, k is the number of principal components you want to retain. You may choose this based on the desired level of explained variance.\n",
    "\n",
    "#### 4. Explained Variance:\n",
    "\n",
    "* Check the explained variance ratio to understand how much variance each principal component captures. This information can help you decide on the appropriate number of components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "583af8bc-0273-4384-8879-c4014d802736",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf5477-7b35-445a-9cb4-1990c981c1f8",
   "metadata": {},
   "source": [
    "#### 5. Choose the Number of Components:\n",
    "\n",
    "* Decide on the number of principal components to retain based on the explained variance. A common approach is to choose a number that retains a sufficiently high percentage of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "#### 6. Transform the Data:\n",
    "\n",
    "* Transform your original dataset using the selected number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18022d0e-2062-42a3-834c-8fbd2f20e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = pca.transform(X_scaled)[:, :selected_components]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d85f7-7796-42ab-9a11-8b6b70d2a477",
   "metadata": {},
   "source": [
    "#### 7. Use the Reduced Dataset for Modeling:\n",
    "\n",
    "* nTrain your stock price prediction model using the reduced dataset.\n",
    "\n",
    "\n",
    "* * PCA helps in reducing the dimensionality of the dataset while retaining as much of the original information as possible. This can lead to improved model performance, reduced overfitting, and faster training times. Keep in mind that while PCA is a powerful tool, interpreting the transformed features might be challenging in terms of the original features' meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e63046b-0187-4de2-85f2-33bb6b97efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example target values for a regression problem\n",
    "y = np.array([10.2, 15.5, 8.9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43ce9452-165e-43c8-bc4c-e2bcb63ceb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [7.70060614e-01 2.29939386e-01 4.86171487e-33]\n",
      "Mean Squared Error: 1.5650247720617625\n"
     ]
    }
   ],
   "source": [
    "# Example code for using PCA for dimensionality reduction in stock price prediction\n",
    "\n",
    "# Step 1: Import necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Load and preprocess your dataset (replace 'your_data.csv' with your actual dataset)\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "# ...\n",
    "\n",
    "# Step 3: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Apply PCA\n",
    "n_components = 3  # Choose the number of components based on your requirements\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 5: Analyze explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Step 6: Select the number of components\n",
    "# Choose based on the explained variance ratio (e.g., 95% cumulative explained variance)\n",
    "selected_components = 2  # Adjust as needed\n",
    "\n",
    "# Step 7: Transform the data\n",
    "X_reduced = pca.transform(X_scaled)[:, :selected_components]\n",
    "\n",
    "# Step 8: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 9: Train a model (example: Linear Regression)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 10: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e906851-88c0-4a55-a0ee-44d24056901e",
   "metadata": {},
   "source": [
    "\n",
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b0c0b-e0a6-4ef6-b4a2-30523769f2ad",
   "metadata": {},
   "source": [
    "* * Min-Max scaling is a common method used to scale and normalize the values of a dataset to a specific range. The formula for Min-Max scaling is:\n",
    "\n",
    "######  scaled = Xscaled= X竏知in(X)/max(X)竏知in(X)*(max range竏知in range)+min range\n",
    "\n",
    "* To transform the values in your dataset to a range of -1 to 1, you can use the following Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adcf07e4-190c-4de8-9daa-c9c3d3703d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: [1, 5, 10, 15, 20]\n",
      "Scaled dataset: [-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaling(data, min_range, max_range):\n",
    "    # Calculate the min and max values of the dataset\n",
    "    data_min = min(data)\n",
    "    data_max = max(data)\n",
    "\n",
    "    # Perform Min-Max scaling\n",
    "    scaled_data = [((x - data_min) / (data_max - data_min)) * (max_range - min_range) + min_range for x in data]\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "# Your dataset\n",
    "dataset = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Define the desired range (-1 to 1)\n",
    "min_range = -1\n",
    "max_range = 1\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_dataset = min_max_scaling(dataset, min_range, max_range)\n",
    "\n",
    "print(\"Original dataset:\", dataset)\n",
    "print(\"Scaled dataset:\", scaled_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b5677-913b-46c3-9ffe-a26494446230",
   "metadata": {},
   "source": [
    "\n",
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c37ac-ab07-4f3a-87b6-ef5728a1cdf3",
   "metadata": {},
   "source": [
    "* Principal Component Analysis (PCA) is a technique used for feature extraction and dimensionality reduction. The goal of PCA is to transform the original features into a new set of uncorrelated features called principal components, ordered by the amount of variance they explain in the data.\n",
    "\n",
    "* To determine the number of principal components to retain, one common approach is to look at the explained variance ratio. The explained variance ratio tells us the proportion of the dataset's variance that lies along each principal component.\n",
    "\n",
    "#### Here's an example using Python and scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f387a39-c0fd-46e5-88fd-78b2735c248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative explained variance: [0.81174187 0.96815046 0.99514312 1.        ]\n",
      "Number of components to retain for 95% variance: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your dataset is a matrix where each row represents an observation and each column represents a feature\n",
    "data = np.array([\n",
    "    [170, 65, 25, 0, 120],\n",
    "    [165, 60, 30, 1, 130],\n",
    "    [180, 70, 35, 0, 110],\n",
    "    [160, 55, 28, 1, 125],\n",
    "    [175, 75, 40, 0, 115]\n",
    "])\n",
    "\n",
    "# Separate features (X) and labels (not used in PCA)\n",
    "X = data[:, :-1]\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X_standardized = (X - mean) / std\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_standardized)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of components to retain (e.g., retaining 95% of the variance)\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Cumulative explained variance:\", cumulative_explained_variance)\n",
    "print(\"Number of components to retain for 95% variance:\", num_components_to_retain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509779b6-b41b-4a8f-8369-e4bf064029da",
   "metadata": {},
   "source": [
    "* In this example, num_components_to_retain will give you the number of principal components to retain in order to capture at least 95% of the variance in the data. You can adjust the threshold (e.g., 95%) based on your specific requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49838b-2877-48ce-9363-0d0587c34c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
